{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    PromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "persist_directory = \"../chroma\"\n",
    "\n",
    "vector_db = Chroma(\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=OpenAIEmbeddings()\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_db._collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Given the developments in Ukraine and the Middle East, I had to see for\\n\\nmyself what the military life is like so I joined the French Foreign Legion.\\n\\nAfter a successful selection, completing one of the toughest military\\n\\ntraining programs in the world and becoming a Legionnaire I think I', metadata={'source': 'C:\\\\Users\\\\richa\\\\Source\\\\Repos\\\\cv_rag\\\\chromadb_etl\\\\src\\\\..\\\\..\\\\data\\\\Richard_Csanaki_CV.txt', 'start_index': 378}),\n",
       " Document(page_content='entire BI pipeline: ETL, data modeling, visualization, reporting and\\n\\nML-based advanced analytics.\\n\\nEmployment History\\n\\nLEGIONNAIRE at FRENCH FOREIGN LEGION, Aubagne\\n\\nNovember 2023 \\x97 March 2024\\n\\nGiven the developments in Ukraine and the Middle East, I had to see for', metadata={'source': 'C:\\\\Users\\\\richa\\\\Source\\\\Repos\\\\cv_rag\\\\chromadb_etl\\\\src\\\\..\\\\..\\\\data\\\\Richard_Csanaki_CV.txt', 'start_index': 183}),\n",
       " Document(page_content='training programs in the world and becoming a Legionnaire I think I\\n\\nhave seen enough.\\n\\nBI DEVELOPER at MAGNA INTERNATIONAL, Graz\\n\\nJune 2021 \\x97 August 2023\\n\\nGlobal HR Reporting:\\n\\nThe first full-fledged BI pipeline utilizing all global HR data', metadata={'source': 'C:\\\\Users\\\\richa\\\\Source\\\\Repos\\\\cv_rag\\\\chromadb_etl\\\\src\\\\..\\\\..\\\\data\\\\Richard_Csanaki_CV.txt', 'start_index': 600})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"french foreign legion\"\n",
    "result = vector_db.similarity_search(question, k=3)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb import Settings\n",
    "\n",
    "client = chromadb.Client(Settings(is_persistent=True, persist_directory=persist_directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_vector_chain = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vector_db.as_retriever(k=3)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-functions-agent', 'lc_hub_commit_hash': 'a1655024b06afbd95d17449f21316291e0726f13dcfaf990cc0d18087ad689a5'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain import hub\n",
    "\n",
    "retrieval_qa_chat_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")\n",
    "llm = ChatOpenAI()\n",
    "retriever = ...\n",
    "combine_docs_chain = create_stuff_documents_chain(\n",
    "    llm, retrieval_qa_chat_prompt\n",
    ")\n",
    "retrieval_chain = create_retrieval_chain(retriever, combine_docs_chain)\n",
    "\n",
    "chain.invoke({\"input\": \"...\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
